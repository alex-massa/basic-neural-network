{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3eadbc6",
   "metadata": {},
   "source": [
    "The `hid_size` parameter controls the number of neurons in the hidden layer of the neural network.\n",
    "\n",
    "The value of `epochs` is the number of times the neural network iterates over the entire training set to adjust its weights.\n",
    "\n",
    "The parameter `learning_rate` controls the velocity at which the neural network converges: generally, the lower the value of `learning_rate`, the more refined the weights used by the neural network to make predictions get; however, lowering this value might require increasing the number of iterations on the training data to obtain good metrics.\n",
    "\n",
    "The `batch_size` parameter affects the number of times the weights in the neural network are updated. \\\n",
    "Let's consider an example: if the training set contains $2000$ samples and the number of iterations is $10$, the neural network will make predictions for all $2000$ samples in one go and thus update its weights exactly $10$ times, which is the number of iterations. \\\n",
    "If a batch size of $100$ is used instead, during each iteration the neural network will make predictions for the first $100$ samples in the training set and adjust its weights, then repeat for the next $100$ samples, and so on, for a total of $\\frac{2000}{100} = 20$ batches per iteration; this means that the neural network will update its weights $20 \\cdot 10 = 200$ times instead of just $10$, which should positively affect its performance.\n",
    "\n",
    "The parameter `skip_remaining` takes a boolean value; if `True`, the last batch will be skipped if the number of its samples is lower than that of `batch_size`. \\\n",
    "For example, if the number of samples is $1003$ and the value of `batch_size` is $10$, then there will be $101$ batches, with the last one containing just $3$ elements instead of $10$; if `skip_remaining` is set to `True`, this last batch will be ignored.\n",
    "\n",
    "The `hid_activation` and `out_activation` parameters are activation functions applied respectively to the values going out of the hidden and the output layer; each function impacts how the network performs and adjusts its weights in a different way. \\\n",
    "The most common activation function is *ReLU*, usually applied to hidden layers, which disables negative weights.\n",
    "\n",
    "The parameter `dropout` accepts a rate within the range $(0, 1]$; dropout is a form of regularization that, during each iteration, disables a different random subset of neurons in a layer; this forces the neural network to train on a different subset of neurons during each epoch. \\\n",
    "For example, if the number of hidden neurons in $100$ and a dropout rate of $0.3$ is applied, then about $30$ neurons will be disabled.\n",
    "\n",
    "The `input_as_boolean` parameter takes a boolean value; if `True`, every non-zero value in the input data will be turned in a $1$. \\\n",
    "This might result useful when working on count data, though not so much when dealing with continuous data.\n",
    "\n",
    "If the `is_generator` parameter if set to `True` the model will yield at the end of each epoch, so that it can be evaluated after every iteration.\n",
    "\n",
    "The parameter `random_seed` allows for reproducible results; if set to `None`, each instance of the neural network will perform differently due to randomness in the initialization of its weights.\n",
    "If the model is robust enough, the results should not differ exaggeratedly between multiple executions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "from functools import wraps\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_generator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if self.is_generator:\n",
    "            return func(self, *args, **kwargs)\n",
    "        for values in func(self, *args, **kwargs):\n",
    "            pass\n",
    "        return values\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Enum):\n",
    "    RELU = lambda x: (x > 0) * x, \\\n",
    "           lambda x: x > 0\n",
    "\n",
    "    SIGMOID = lambda x: 1 / (1 + np.exp(-x)), \\\n",
    "              lambda x: x * (1 - x)\n",
    "\n",
    "    TANH = lambda x: np.tanh(x), \\\n",
    "           lambda x: 1 - (x ** 2),\n",
    "\n",
    "    SOFTMAX = lambda x: (ex := np.exp(x)) / np.sum(ex, axis=1, keepdims=True), \\\n",
    "              lambda x: (_ for _ in ()).throw(Exception(\"activation function softmax only works on output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuralNetwork:\n",
    "\n",
    "    def __init__(self, hid_size, epochs=3, learning_rate=1e-3, batch_size=32, skip_remaining=True,\n",
    "                 hid_activation=None, out_activation=None, dropout=1,\n",
    "                 input_as_boolean=False, is_generator=False, random_seed=None):\n",
    "        self.hid_size = hid_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_remaining = skip_remaining\n",
    "        self.hid_activation = hid_activation\n",
    "        self.out_activation = out_activation\n",
    "        self.dropout = dropout\n",
    "        self.input_as_boolean = input_as_boolean\n",
    "        self.is_generator = is_generator\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if self.hid_activation is not None:\n",
    "            self.__hid_activation_fun, self.__hid_activation_deriv = self.hid_activation.value\n",
    "        if self.out_activation is not None:\n",
    "            self.__out_activation_fun, _ = self.out_activation.value\n",
    "\n",
    "        self.__rng = np.random.default_rng(seed=self.random_seed)\n",
    "\n",
    "    @property\n",
    "    def input_to_hidden_coeffs(self):\n",
    "        return copy(self._coeffs_in_to_hid)\n",
    "\n",
    "    @property\n",
    "    def hidden_to_output_coeffs(self):\n",
    "        return copy(self._coeffs_hid_to_out)\n",
    "\n",
    "    @conditional_generator\n",
    "    def fit(self, train_samples, train_labels):\n",
    "        assert(len(train_samples) == len(train_labels))\n",
    "        # initialize coefficients with values between -0.1 and 0.1\n",
    "        self._coeffs_in_to_hid = 0.2 * self.__rng.random((len(train_samples.T), self.hid_size)) - 0.1\n",
    "        self._coeffs_hid_to_out = 0.2 * self.__rng.random((self.hid_size, len(train_labels.T))) - 0.1\n",
    "        if self.input_as_boolean:\n",
    "            train_samples = np.array(train_samples).astype(bool)\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(0, len(train_samples), self.batch_size):\n",
    "                samples = train_samples[i:i + self.batch_size]\n",
    "                labels = train_labels[i:i + self.batch_size]\n",
    "                batch_size = len(samples)\n",
    "                if batch_size < self.batch_size and self.skip_remaining:\n",
    "                    continue\n",
    "\n",
    "                dropout_mask = self.__rng.choice((0, 1), size=(batch_size, self.hid_size),\n",
    "                                                 p=(self.dropout, 1 - self.dropout))\n",
    "\n",
    "                layers_in = samples\n",
    "                layers_hid = layers_in.dot(self._coeffs_in_to_hid)\n",
    "                if self.hid_activation is not None:\n",
    "                    layers_hid = self.__hid_activation_fun(layers_hid)\n",
    "                layers_hid *= dropout_mask * (1 / self.dropout)\n",
    "                layers_out = layers_hid.dot(self._coeffs_hid_to_out)\n",
    "                if self.out_activation is not None:\n",
    "                    layers_out = self.__out_activation_fun(layers_out)\n",
    "\n",
    "                deltas_out = labels - layers_out\n",
    "                deltas_hid = deltas_out.dot(self._coeffs_hid_to_out.T)\n",
    "                if self.hid_activation is not None:\n",
    "                    deltas_hid *= self.__hid_activation_deriv(layers_hid)\n",
    "                deltas_hid *= dropout_mask\n",
    "\n",
    "                self._coeffs_hid_to_out += layers_hid.T.dot(deltas_out) * self.learning_rate\n",
    "                self._coeffs_in_to_hid += layers_in.T.dot(deltas_hid) * self.learning_rate\n",
    "            yield self\n",
    "\n",
    "    def predict(self, samples, normalize=False):\n",
    "        assert(len(samples.T) == len(self._coeffs_in_to_hid))\n",
    "        if self.input_as_boolean:\n",
    "            samples = np.array(samples).astype(bool)\n",
    "        layers_in = samples\n",
    "        layers_hid = layers_in.dot(self._coeffs_in_to_hid)\n",
    "        if self.hid_activation is not None:\n",
    "            layers_hid = self.__hid_activation_fun(layers_hid)\n",
    "        layers_out = layers_hid.dot(self._coeffs_hid_to_out)\n",
    "        if self.out_activation is not None:\n",
    "            layers_out = self.__out_activation_fun(layers_out)\n",
    "        if normalize and self.out_activation is not Activation.SOFTMAX:\n",
    "            to_probs, _ = Activation.SOFTMAX.value\n",
    "            layers_out = to_probs(layers_out)\n",
    "        return layers_out\n",
    "\n",
    "    def evaluate(self, samples, labels):\n",
    "        assert(len(samples) == len(labels))\n",
    "        assert(len(labels.T) == len(self._coeffs_hid_to_out.T))\n",
    "        preds = self.predict(samples, normalize=False)\n",
    "        errors = ((labels - preds) ** 2).sum(axis=0)\n",
    "        loss = sum(errors) / len(preds)\n",
    "        n_correct = sum([np.argmax(pred) == np.argmax(label)\n",
    "                        for pred, label in zip(preds, labels)])\n",
    "        accuracy = n_correct / len(preds)\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSparseNeuralNetwork:\n",
    "\n",
    "    def __init__(self, n_total, hid_size, epochs=3, learning_rate=1e-3, batch_size=32, skip_remaining=True,\n",
    "                 hid_activation=None, out_activation=None, dropout=1,\n",
    "                 input_as_boolean=False, is_generator=False, random_seed=None):\n",
    "\n",
    "        self.n_total = n_total\n",
    "        self.hid_size = hid_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_remaining = skip_remaining\n",
    "        self.hid_activation = hid_activation\n",
    "        self.out_activation = out_activation\n",
    "        self.dropout = dropout\n",
    "        self.input_as_boolean = input_as_boolean\n",
    "        self.is_generator = is_generator\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if self.hid_activation is not None:\n",
    "            self.__hid_activation_fun, self.__hid_activation_deriv = self.hid_activation.value\n",
    "        if self.out_activation is not None:\n",
    "            self.__out_activation_fun, _ = self.out_activation.value\n",
    "\n",
    "        self.__rng = np.random.default_rng(seed=self.random_seed)\n",
    "\n",
    "    @property\n",
    "    def input_to_hidden_coeffs(self):\n",
    "        return copy(self._coeffs_in_to_hid)\n",
    "\n",
    "    @property\n",
    "    def hidden_to_output_coeffs(self):\n",
    "        return copy(self._coeffs_hid_to_out)\n",
    "\n",
    "    @conditional_generator\n",
    "    def fit(self, train_samples, train_labels):\n",
    "        assert(len(train_samples) == len(train_labels))\n",
    "\n",
    "        # initialize coefficients using values between -0.1 and 0.1\n",
    "        self._coeffs_in_to_hid = 0.2 * self.__rng.random((self.n_total, self.hid_size)) - 0.1\n",
    "        self._coeffs_hid_to_out = 0.2 * self.__rng.random((self.hid_size, len(train_labels.T))) - 0.1\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(0, len(train_samples), self.batch_size):\n",
    "                samples = train_samples[i:i + self.batch_size]\n",
    "                labels = train_labels[i:i + self.batch_size]\n",
    "                batch_size = len(samples)\n",
    "                if batch_size < self.batch_size and self.skip_remaining:\n",
    "                    continue\n",
    "\n",
    "                dropout_mask = self.__rng.choice((0, 1), size=(batch_size, self.hid_size),\n",
    "                                                 p=(self.dropout, 1 - self.dropout))\n",
    "\n",
    "                layers_in, layers_hid = [], []\n",
    "                for j in range(batch_size):\n",
    "                    layer_in = samples[j]\n",
    "                    if not self.input_as_boolean:\n",
    "                        layer_hid = layer_in.T[1].dot(self._coeffs_in_to_hid[layer_in.T[0]])\n",
    "                    else:\n",
    "                        layer_hid = self._coeffs_in_to_hid[layer_in.T[0]].sum(axis=0)\n",
    "                    layers_in.append(layer_in), layers_hid.append(layer_hid)\n",
    "\n",
    "                layers_hid = np.array(layers_hid)\n",
    "                if self.hid_activation is not None:\n",
    "                    layers_hid = self.__hid_activation_fun(layers_hid)\n",
    "                layers_hid *= dropout_mask * (1 / self.dropout)\n",
    "                layers_out = layers_hid.dot(self._coeffs_hid_to_out)\n",
    "                if self.out_activation is not None:\n",
    "                    layers_out = self.__out_activation_fun(layers_out)\n",
    "\n",
    "                deltas_out = labels - layers_out\n",
    "                deltas_hid = deltas_out.dot(self._coeffs_hid_to_out.T)\n",
    "                if self.hid_activation is not None:\n",
    "                    deltas_hid *= self.__hid_activation_deriv(layers_hid)\n",
    "                deltas_hid *= dropout_mask\n",
    "\n",
    "                self._coeffs_hid_to_out += layers_hid.T.dot(deltas_out) * self.learning_rate\n",
    "                for j in range(batch_size):\n",
    "                    self._coeffs_in_to_hid[layers_in[j].T[0]] += deltas_hid[j] * self.learning_rate\n",
    "            yield\n",
    "\n",
    "    def predict(self, samples, normalize=False):\n",
    "        layers_hid = []\n",
    "        for sample in samples:\n",
    "            layer_in = sample\n",
    "            if not self.input_as_boolean:\n",
    "                layer_hid = layer_in.T[1].dot(self._coeffs_in_to_hid[layer_in.T[0]])\n",
    "            else:\n",
    "                layer_hid = self._coeffs_in_to_hid[layer_in.T[0]].sum(axis=0)\n",
    "            if self.hid_activation is not None:\n",
    "                layer_hid = self.__hid_activation_fun(layer_hid)\n",
    "            layers_hid.append(layer_hid)\n",
    "\n",
    "        layers_hid = np.array(layers_hid)\n",
    "        layers_out = layers_hid.dot(self._coeffs_hid_to_out)\n",
    "        if self.out_activation is not None:\n",
    "            layers_out = self.__out_activation_fun(layers_out)\n",
    "        if normalize and self.out_activation is not Activation.SOFTMAX:\n",
    "            to_probs, _ = Activation.SOFTMAX.value\n",
    "            layers_out = to_probs(layers_out)\n",
    "        return layers_out\n",
    "\n",
    "    def evaluate(self, samples, labels):\n",
    "        assert(len(samples) == len(labels))\n",
    "        assert(len(labels.T) == len(self._coeffs_hid_to_out.T))\n",
    "        preds = self.predict(samples, normalize=False)\n",
    "        errors = ((labels - preds) ** 2).sum(axis=0)\n",
    "        loss = sum(errors) / len(preds)\n",
    "        n_correct = sum([np.argmax(pred) == np.argmax(label)\n",
    "                        for pred, label in zip(preds, labels)])\n",
    "        accuracy = n_correct / len(preds)\n",
    "        return loss, accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
